== Gestion des données

Vous pouvez consulter notre documentation sur l'api REST 'rest-api.pdf' pour connaitre les détails de chaque fonction de l'API (endpoint).
Maintenant que nous avons installé l'historian. Vous pouvez jouer avec des données, il y a plusieurs manières
d'interagir avec l'historian selon votre culture et vos besoins.

L'historian ne contient initialement aucune données. Il existe plusieurs moyens d'injecter des données (voir le guide sur l'api rest),
dans ce guide nous allons utiliser l'import de données à partir de fichiers csv.

=== Import de donnée à partir de fichiers csv avec l'api REST

Cette section va nous apprendre à injecter un fichier CSV dans l'historian. Le folder **samples** du data historian contient des exemples. Nous allons injecter les données du sous folder **happiness** ; quelques fichiers qui donnent le score de "happiness" des pays chaque année.

Excecutez la commande suivante (dans ce qui suit remplacez /opt/hdp par votre directorie d'installation si vous n'avez pas installé dans la directorie par défaut): 

[source,bash]
----
curl -X POST \
http://localhost:8080/api/historian/v0/import/csv \
-F 'my_csv_file=@/opt/hdh/samples/happiness/2015.csv' \
-F mapping.value="Happiness Score" \
-F mapping.timestamp=Timestamp \
-F mapping.tags=Country \
-F group_by=name \
-F group_by=tags.Country \
-F format_date='yyyy-MM-dd HH:mm:ss'
----

et bien sûr vous pouvez appliquer la même requête à tous les fichiers de 2016 à 2020. Cela vous fait un petit jeu de données sur lequel nous allons pouvoir apprendre à faire des choses.

=== Requêter des données avec l'api REST

Une manière de voir si vous données ont bien été injectées est de les requêter. Essayons de voir
la progression du bonheur (métrique happiness) en France sur les années 2015 à 2020. Et essayons de l'obtenir en Json.

Déjà regardons simplement si il existe des métriques happiness dans l'historian avec la requête suivante:

[source,bash]
----
curl --header 'Content-Type: application/json' -X POST http://localhost:8080/api/grafana/v0/query -d '{
  "names": ["happiness"],
  "format" : "json"
}'
----

En effet nous obtenons des données.

En filtrant sur le tag Country (= France) nous devrions avoir les enregistrements de la France uniquement.


[source,bash]
----
curl --header 'Content-Type: application/json' -X POST http://localhost:8080/api/grafana/v0/query -d '{
  "names": ["happiness"],
  "Country: "France",
  "format" : "json"
}'
----

Obtenir l'ensemble des noms de métriques

[source,bash]
----
curl --location --request POST 'http://localhost:8080/api/grafana/v0/search' \
--header 'Content-Type: application/json' \
--data-raw '{
    "target": "*"
}'
----

Obtenir des mesures avec en spécifiant un intervalle de temps:

[source,bash]
----
curl --location --request POST 'http://localhost:8080/api/grafana/v0/query' \
--header 'Content-Type: application/json' \
--data-raw '{
	 "range": {
          "from": "2015-01-01 00:00:00",
          "to": "2016-01-01 00:00:00"
      },
      "targets": [
        { "target": "ack" }
      ]
}'
----

=== Visualisez les données dans Grafana

En général vous voudez visualiser vos données plutôt que de les requêter. Allez sur l'URL de grafana (http://localhost:3000)[http://localhost:3000]. La première fois que vous vous loggez vous pouvez entrer n'importe quel username et mot de passe (celui ci deviendra le premier utilisateur et administrateur). Vous pouvez mettre admin, admin (dans l'écran suivant il vous sera demandé de changer ce mot de passe "admin" en quelque chose de plus sécurisé).

Une fois votre mot de passe changé, vous avez une page d'accueil que nous pouvez lire puis faire disparaître.

- Vous allez clicker sur "Add your first data source".
- Aller à la fin clicker sur celle qui s'appelle "Hurence-Historian".
- Mettez dans URL l'URL de votre historian server (l'installation par défaut est http://localhost:8080/api/grafana/v0/)
- Clickez "Save and Test"
- Vous devez avoir un message vert vous disant que votre data source est chargée.



=== Utiliser Spark pour requếter les données

Apache Spark est une plateforme open source pour traiter de grandes quantités de données en parallèle sur un cluster de machines.

Hurence Data Historian est très intégré avec Spark de telle sorte que vous puissiez gérer des intéractions avec les données (en entrées pour en injecter ou en requêtage pour les obtenir sur des critères), et ceci avec une API Spark simple.

Les commandes suivantes vous montrent comment prendre un dataset en CSV depuis HDFS (le système de fichiers de Hadoop) ou depuis un système de fichier local, et le charger dans HDH.

[source,bash]
----
$HDH_HOME/spark-{spark_version}-bin-hadoop{hadoop_version}/bin/spark-shell --jars assembly/target/historian-{hdh_version}-SNAPSHOT/historian-{hdh_version}-SNAPSHOT//lib/loader-{hdh_version}-SNAPSHOT.jar,assembly/target/historian-{hdh_version}-SNAPSHOT/historian-{hdh_version}-SNAPSHOT/lib/spark-solr-3.6.6-shaded.jar
----

[source,scala]
----
import com.hurence.historian.model.ChunkRecordV0
import com.hurence.historian.spark.ml.Chunkyfier
import com.hurence.historian.spark.sql
import com.hurence.historian.spark.sql.functions._
import com.hurence.historian.spark.sql.reader.{MeasuresReaderType, ReaderFactory}
import com.hurence.historian.spark.sql.writer.{WriterFactory, WriterType}
import com.lucidworks.spark.util.SolrSupport
import org.apache.commons.cli.{DefaultParser, Option, Options}
import org.apache.spark.sql.SparkSession
import org.slf4j.LoggerFactory

 val filePath = "/Users/tom/Documents/workspace/historian/loader/src/test/resources/it-data-4metrics.csv.gz"
 val reader = ReaderFactory.getMeasuresReader(MeasuresReaderType.GENERIC_CSV)
    val measuresDS = reader.read(sql.Options(
      filePath,
      Map(
        "inferSchema" -> "true",
        "delimiter" -> ",",
        "header" -> "true",
        "nameField" -> "metric_name",
        "timestampField" -> "timestamp",
        "timestampDateFormat" -> "s",
        "valueField" -> "value",
        "tagsFields" -> "metric_id,warn,crit"
      )))



val chunkyfier = new Chunkyfier().setGroupByCols(Array(  "name", "tags.metric_id"))
val chunksDS = chunkyfier.transform(measuresDS).as[ChunkRecordV0]


val writer = WriterFactory.getChunksWriter(WriterType.SOLR)
    writer.write(sql.Options("historian", Map(
      "zkhost" -> "localhost:9983",
      "collection" -> "historian",
      "tag_names" -> "metric_id,warn,crit"
    )), chunksDS)

----

=== L'injection temps réel avec LogIsland

Le logiciel Open Source de Hurence permettant de faire du stream processing (donc du traitement temps réel de données) permet bien sûr d'injecter des données "à la volée", notamment celles que vous aurez pu "pousser" dans un bus de message Mqtt ou Kafka.

Hurence Data Historian est donc capable de traiter des données de capteurs et de les stocker et grapher en temps réel.

Pour mettre en place une chaîne d'injection temps réel, le mieux est de contacter Hurence pour un petit accompagnement car cela devient du vrai Big Data temps réel.

== Visualisation des données

HDH fournit donc un plugin Grafana pour visualiser les données sous formes de graphes. 

La manipulation des données depuis les data sources de votre data historian se fait comme pour les autres sources. Vous fournissez votre requête, un intervalle de temps si nécessaire, et un algorithme de sampling si vous requêtez sur une grande quantité de points (pour que la courbe soit bien lisible).

=== Configurer Grafana et vos dashboard

Pour créér des dashboard il faut aller sur l'interface graphique de grafana (http://localhost:3000/[http://localhost:3000/] (localhost étant à remplacer par un nom de machine si vous n'êtes pas en installation standalone).

Allez ensuite dans le menu des datasources :

image::grafana_ui_datasource.png[go to datasource page]

Cherchez la datasource "Hurence-Historian" et sélectionnez la.
Il y a juste a renseigner l'URL http://localhost:8080/api/grafana/v0 dans le cas de l'installation standalone.

image::datasource_historian_setup.png[datasource setup]

Testez la connectivité en cliquant sur "Save & Test" button at the bottom of the page.
Ensuite créez un nouveau dashboard.

image::grafana_ui_create_dashboard.png[go to new dashboard page]

Ajoutez les graphes que vous voulez, un petit exemple ci-dessous :

image::grafana_graph_config_exemple.png[exemple of query config]

Une "query" (requête) consiste à renseigner un nom de métrique, et des paires de tag name/tag value ainsi que des options pour l'algorithme de sampling.

note:: les options de sampling de la première requête sont utilisées pour toutes les autres.

note:: si vous n'avez pas de données d'injectées, suivez le tutorial pour injecter des données.

note:: si vous n'avez pas ajouté de tag name à l'installation vous ne pourrez pas utiliser de tags. Il est toujours possible de rajouter des tags manuellement après installation en rajoutant un champ dans le schéma.
