== Gestion des données

Vous pouvez consulter notre documentation sur l'api REST {url-rest-api-pdf}[api rest] pour connaitre les détails de chaque fonction de l'API (endpoint).
Maintenant que nous avons installé l'historian. Vous pouvez jouer avec des données, il y a plusieurs manières
d'interagir avec l'historian selon votre culture et vos besoins.

L'historian ne contient initialement aucune données. Il existe plusieurs moyens d'injecter des données (voir le guide sur l'api rest),
dans ce guide nous allons utiliser l'import de données à partir de fichiers csv.

=== Import de donnée à partir de fichiers csv avec l'api REST

Cette section va nous apprendre à injecter un fichier CSV dans l'historian. Le folder **samples** du data historian contient des exemples. Nous allons injecter les données du sous folder **happiness** ; quelques fichiers qui donnent le score de "happiness" des pays chaque année.

Excecutez la commande suivante (dans ce qui suit remplacez $HDH_HOME par votre répertoire d'installation et éventuellement l'adresse de votre serveur historian si vous n'avez pas effectué une installation locale ):

[source,bash, subs="attributes"]
----
curl -X POST \
http://localhost:8080/api/historian/v0/import/csv \
-F 'my_csv_file=@$HDH_HOME/historian-{hdh_version}/samples/happiness/2015.csv' \
-F mapping.value="Happiness Score" \
-F mapping.timestamp=Timestamp \
-F mapping.tags=Country \
-F group_by=name \
-F group_by=tags.Country \
-F format_date='yyyy-MM-dd HH:mm:ss'
----

et bien sûr vous pouvez appliquer la même requête à tous les fichiers de 2016 à 2020. Cela vous fait un petit jeu de données sur lequel nous allons pouvoir apprendre à faire des choses.

Ces query devraient retourner une réponse de ce type :

[source,json]
----
{
  "tags" : [ "Country" ],
  "grouped_by" : [ "name", "Country" ],
  "report" : [ {
    "name" : "happiness",
    "Country" : "Switzerland",
    "number_of_points_injected" : 1,
    "number_of_point_failed" : 0,
    "number_of_chunk_created" : 1
  },
  ...,{
    "name" : "happiness",
    "Country" : "Togo",
    "number_of_points_injected" : 1,
    "number_of_point_failed" : 0,
    "number_of_chunk_created" : 1
  }]
}
----

=== Requêter des données avec l'api REST

Une manière de voir si vous données ont bien été injectées est de les requêter. Essayons de voir
la progression du bonheur (métrique happiness) en France sur les années 2015 à 2020. Et essayons de l'obtenir en Json.

Déjà regardons simplement si il existe des métriques happiness dans l'historian avec la requête suivante:

[source,bash]
----
curl --header 'Content-Type: application/json' -X POST \
http://localhost:8080/api/grafana/v0/query -d '{
  "names": ["happiness"]
}'
----

Cette query retourne une réponse de ce type :

[source,json]
----
[{
   "name":"happiness",
   "datapoints":[[7.587,1420070400000],...,[2.566900015,1577836800000]]
}]
----


En filtrant sur le tag Country (= France) nous devrions avoir les enregistrements de la France uniquement.

[source,bash]
----
curl --header 'Content-Type: application/json' -X POST \
http://localhost:8080/api/grafana/v0/query -d '{
  "names": ["happiness"],
  "tags": {
     "Country": "France"
  }
}'
----

Cette query retourne la réponse suivante :

[source,json]
----
[{
  "name":"happiness",
  "tags":{"Country":"France"},
  "datapoints":[
    [6.575,1420070400000],
    [6.478,1451606400000],
    [6.44199991226196,1483228800000],
    [6.489,1514764800000],
    [6.592,1546300800000],
    [6.663799763,1577836800000]
   ]
}]
----

Obtenir l'ensemble des noms de métriques.

[source,bash]
----
curl --X POST 'http://localhost:8080/api/grafana/v0/search' \
--header 'Content-Type: application/json' \
--data-raw '{
  "name": "ha",
  "limit": 100
}'
----

Ici on précise que l'on désire avoir toutes les noms de métrique contenant la string "ha" mais de n'en retrouner qu'un maximum de 100.
Si l'on ne renseigne pas le champ "name" alors la requête retourne tout les noms de métrique.

[source,json]
----
["happiness"]
----


Obtenir l'ensemble des tags présent dans l'historian.

[source,bash]
----
curl -X POST 'http://localhost:8080/api/grafana/v0/search/tags'
----

Cette query retourne la liste des tags présent en json :

[source,json]
----
["Country"]
----

Obtenir l'ensemble des valeurs d'un tag.

[source,bash]
----
curl -X POST 'http://localhost:8080/api/grafana/v0/search/values' \
--header 'Content-Type: application/json' \
--data-raw '{
  "field": "Country",
  "query": "F",
  "limit": 100
}'
----

Ici on précise que l'on désire avoir toutes les valeurs existante pour le tag "Country" contenant la string "F" mais de n'en retrouner qu'un maximum de 100.
Si l'on ne renseigne pas le champ "name" alors la requête retourne tous les noms de métrique.

Cette query retourne cette liste :

[source,json]
----
["Burkina Faso","Finland","France"]
----

Attention la requête est sensible aux majuscules. La même requête avec un "f" minuscule retournera en effet :

[source,json]
----
["Afghanistan","Central African Republic","Hong Kong S.A.R. of China","South Africa","Taiwan Province of China"]
----

Obtenir des mesures en spécifiant :

* Un intervalle de temps
* La métrique dont on chercher les données est "happiness"
* On veut au maximum 10 points
* Si il y a besoin d'effectuer un sampling nous voulons utiliser un algorithme à base de bucket et en prenant la plus petite valeur à chaque fois.

[source,bash]
----
curl -X POST 'http://localhost:8080/api/grafana/v0/query' \
--header 'Content-Type: application/json' \
--data-raw '{
    "from": "2015-01-01T00:00:00.000Z",
    "to": "2016-01-01T00:00:00.000Z",
    "names": ["happiness"],
    "max_data_points": 10,
    "sampling":{
       "algorithm": "MIN",
       "bucket_size" : 100
    }
}'
----

Cette query retourne cette réponse :

[source,json]
----
[{
  "name":"happiness",
  "datapoints":[
    [4.867,1420070400000],
    [2.839,1420070400000],
    [3.36,1451606400000],
    [2.905,1451606400000]
   ]
}]
----

Nous obtenons bien moins de 10 points. Attention comme nous n'avons pas précisé de tags les valeurs de tous les pays sont mélangés !

note:: Il n y a ici que 4 points de retournés alors que nous avons spécifié un maximum de 10 points. Cela n'est pas optimum car il
y a plus de 10 points en tout ! En effet nous devrions ici avoir 10 points. Ce problème sera fixé lors de la prochaine release. En attendant
n'hésitez pas à augmenter la valeur de "max_data_points" si cela devait vous arriver.

warn:: Attention cependant a ne pas demander trop de points ! En effet si vous voulez exporter la totalité des points cette endpoint n'est
pas l'outil adéquat. D'ailleurs si le nombre indiqué est supérieur à {max_data_points_max} la requête ne fonctionnera pas. Dans une futur release
le maximum pour cette variable sera revue à la baisse.

=== Utiliser Spark pour requếter les données

Apache Spark est une plateforme open source pour traiter de grandes quantités de données en parallèle sur un cluster de machines.

Hurence Data Historian est très intégré avec Spark de telle sorte que vous puissiez gérer des intéractions avec les données (en entrées pour en injecter ou en requêtage pour les obtenir sur des critères), et ceci avec une API Spark simple.

Les commandes suivantes vous montrent comment prendre un dataset en CSV depuis HDFS (le système de fichiers de Hadoop) ou depuis un système de fichier local, et le charger dans HDH.

[source,bash, subs="attributes"]
----
$HDH_HOME/spark-{spark_version}-bin-hadoop{hadoop_version}/bin/spark-shell --jars assembly/target/historian-{hdh_version}-SNAPSHOT/historian-{hdh_version}-SNAPSHOT//lib/loader-{hdh_version}-SNAPSHOT.jar,assembly/target/historian-{hdh_version}-SNAPSHOT/historian-{hdh_version}-SNAPSHOT/lib/spark-solr-3.6.6-shaded.jar
----

[source,scala]
----
import com.hurence.timeseries.model.Chunk
import com.hurence.historian.spark.ml.Chunkyfier
import com.hurence.historian.spark.sql
import org.apache.spark.sql._
import com.hurence.historian.spark.sql.functions._
import com.hurence.historian.spark.sql.reader.{ChunksReaderType, MeasuresReaderType, ReaderFactory}
import com.hurence.historian.spark.sql.writer.{WriterFactory, WriterType}
import com.lucidworks.spark.util.SolrSupport
import org.apache.commons.cli.{DefaultParser, Option, Options}
import org.apache.spark.sql.SparkSession
import org.slf4j.LoggerFactory

val filePath = "/Users/tom/Documents/workspace/historian/historian-spark/src/test/resources/it-data-4metrics.csv.gz"
val reader = ReaderFactory.getMeasuresReader(MeasuresReaderType.GENERIC_CSV)
val measuresDS = reader.read(sql.Options(
  filePath,
  Map(
    "inferSchema" -> "true",
    "delimiter" -> ",",
    "header" -> "true",
    "nameField" -> "metric_name",
    "timestampField" -> "timestamp",
    "timestampDateFormat" -> "s",
    "valueField" -> "value",
    "tagsFields" -> "metric_id,warn,crit"
  )))



val chunkyfier = new Chunkyfier().setGroupByCols(Array(  "name", "tags.metric_id")).setSaxStringLength(20)
val chunksDS = chunkyfier.transform(measuresDS).as[Chunk](Encoders.bean(classOf[Chunk]))


val writer = WriterFactory.getChunksWriter(WriterType.SOLR)
writer.write(sql.Options("historian", Map(
  "zkhost" -> "localhost:9983",
  "collection" -> "historian",
  "tag_names" -> "metric_id,warn,crit"
)), chunksDS)


// 4. Explicit commit to make sure all docs are visible
val solrCloudClient = SolrSupport.getCachedCloudClient("localhost:9983")
solrCloudClient.commit("historian", true, true)


     val reader = ReaderFactory.getChunksReader(ChunksReaderType.SOLR)
    val solrDF = reader.read(sql.Options("historian", Map(
        "zkhost" -> "localhost:9983",
        "collection" -> "historian",
        "tag_names" -> "metric_id,warn,crit"
    ))).as[Chunk](Encoders.bean(classOf[Chunk]))
----

=== L'injection temps réel avec LogIsland

Le logiciel Open Source de Hurence permettant de faire du stream processing (donc du traitement temps réel de données) permet bien sûr d'injecter des données "à la volée", notamment celles que vous aurez pu "pousser" dans un bus de message Mqtt ou Kafka.

Hurence Data Historian est donc capable de traiter des données de capteurs et de les stocker et grapher en temps réel.

Pour mettre en place une chaîne d'injection temps réel, le mieux est de contacter Hurence pour un petit accompagnement car cela devient du vrai Big Data temps réel.

== Visualisation des données

HDH fournit donc un plugin Grafana pour visualiser les données sous formes de graphes. 

La manipulation des données depuis les data sources de votre data historian se fait comme pour les autres sources. Vous fournissez votre requête, un intervalle de temps si nécessaire, et un algorithme de sampling si vous requêtez sur une grande quantité de points (pour que la courbe soit bien lisible).

=== Configurer la data source grafana

En général vous voudez visualiser vos données plutôt que de les requêter. Allez sur {url-grafana}[l'URL de grafana]. Vous pouvez vous logger en tant que user: admin et pwd: admin (Vous pouvez passer l'écran suivant).

Vous avez une page d'accueil que nous pouvez lire puis faire disparaître.
Afin de pouvoir consulter les données du data historian :

==== Explication sans image

1. Vous allez clicker sur le bouton de Configuration à gauche puis sur "Data Sources".
2. Clickez sur le bouton "Add data source".
3. Aller à la fin et clickez sur celle qui s'appelle "Hurence-Historian" ou bien filtrer dans la barre de recherche pour la faire apparaître.
4. Mettez dans la case "URL" l'URL de votre historian server (l'installation par défaut est {url-historian-grafana-api})
5. Clickez "Save and Test"
6. Vous devez avoir un message vert vous disant que votre data source est chargée et bien configuré (joignable).

==== Explication avec image

Allez dans le menu des datasources :

image::grafana_ui_datasource.png[go to datasource page]

Cherchez la datasource "Hurence-Historian" et sélectionnez la.
Il y a juste a renseigner l'URL {url-historian-grafana-api} dans le cas de l'installation standalone.

image::datasource_historian_setup.png[datasource setup]

Testez la connectivité en cliquant sur "Save & Test" button at the bottom of the page.

=== Configurer vos dashboard

Pour créér des dashboards il faut aller sur l'interface graphique de grafana {url-grafana}[l'URL de grafana] (localhost étant à remplacer par un nom de machine si vous n'êtes pas en installation standalone).

Ensuite créez un nouveau dashboard.

image::grafana_ui_create_dashboard.png[go to new dashboard page]

Ajoutez les graphes que vous voulez, un petit exemple ci-dessous :

* Nous avons sélectionné la métrique "happiness" (préalablement injecté).
* La time range choisis est de "2014-12-18 21:08:01" à "2020-01-13 14:48:41".

image::grafana_dashboard_all_happiness.png[exemple of query config]

Pour configurer la time range :

image::grafana_dashboard_all_happiness_timerange.png[exemple of query config]

TODO remarque sur la visu

Si on filtre sur le tag "Country" avec la valeur France par exemple on obtient :

image::grafana_dashboard_france_happiness.png[exemple of query config]

Pour cela il faut cliquer sur "Add new tag filter". Puis sélectionner la valeur "Country" comme "Tag name".
Enfin cliquer sur "Tag value" et commencer à taper le nom du pays désirer puis sélectionner "France" lorsque celui-ci apparaît.

image::grafana_dashboard_add_tag.png[exemple of query config]


TODO général note

Une "query" (requête) consiste à renseigner un nom de métrique, et des paires de tag name/tag value ainsi que des options pour l'algorithme de sampling.

note:: les options de sampling de la première requête sont utilisées pour toutes les autres.

note:: si vous n'avez pas de données d'injectées, suivez le tutorial pour injecter des données.

note:: si vous n'avez pas ajouté de tag name à l'installation vous ne pourrez pas utiliser de tags. Il est toujours possible de rajouter des tags manuellement après installation en rajoutant un champ dans le schéma.
