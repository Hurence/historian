== Concepts

Hurence Data Historian is a free solution to handle massive loads of timeseries data into a search engine (such as Apache SolR).
The key concepts are simple :

- **Measure** is a point in time with a floating point value identified by a name and some tags (categorical features)
- **Chunk** is a set of contiguous Measures with a time interval grouped by a date bucket, the measure name and eventually some tags

The main purpose of this tool is to help creating, storing and retrieving these chunks of timeseries.
We use chunking instead of raw storage in order to save some disk space and reduce costs at scale. Also chunking is very usefull to pre-compute some aggregation and to facilitate down-sampling

=== Data model

A Measure point is identified by the following fields. Tags are used to add meta-information.
[source,java]
----
class Measure  {
    String name;
    long timestamp;
    double value;
    float quality;
    Map<String, String> tags;
}
----


A Chunk is identified by the following fields
[source,java]
----
class Chunk  {
    SchemaVersion version = SchemaVersion.VERSION_1;
    String name, id, metricKey;
    byte[] value;
    long start, end;
    Map<String, String> tags;

    long count, first, min, max, sum, avg, last, stdDev;
    float qualityFirst, qualityMin, qualityMax, qualitySum, qualityAvg;

    int year, month;
    String day;
    String origin;
    String sax;
    boolean trend;
    boolean outlier;
}
----

As you can see from a Measure points to a Chunk of Measures, the `timestamp` field has been replaced by a `start` and `stop` interval and the `value` is now a base64 encoded string named `chunk`.


In SolR the chunks will be stored according to  the following schema (for the current version)

[source,xml]
----
<schema name="default-config" version="1.6">
    ...
  <field name="chunk_avg" type="pdouble"/>
  <field name="chunk_count" type="pint"/>
  <field name="chunk_day" type="string"/>
  <field name="chunk_end" type="plong"/>
  <field name="chunk_first" type="pdouble"/>
  <field name="chunk_hour" type="pint"/>
  <field name="chunk_last" type="pdouble"/>
  <field name="chunk_max" type="pdouble"/>
  <field name="chunk_min" type="pdouble"/>
  <field name="chunk_month" type="pint"/>
  <field name="chunk_origin" type="string"/>
  <field name="chunk_outlier" type="boolean"/>
  <field name="chunk_quality_avg" type="pfloat"/>
  <field name="chunk_quality_first" type="pfloat"/>
  <field name="chunk_quality_max" type="pfloat"/>
  <field name="chunk_quality_min" type="pfloat"/>
  <field name="chunk_quality_sum" type="pfloat"/>
  <field name="chunk_sax" type="ngramtext"/>
  <field name="chunk_start" type="plong"/>
  <field name="chunk_std_dev" type="pdouble"/>
  <field name="chunk_sum" type="pdouble"/>
  <field name="chunk_trend" type="boolean"/>
  <field name="chunk_value" type="text_general" multiValued="false" indexed="false"/>
  <field name="chunk_year" type="pint"/>
  <field name="metric_id" type="string" docValues="true" multiValued="false" indexed="true" stored="true"/>
  <field name="metric_key" type="string"/>
  <field name="name" type="string" multiValued="false" indexed="true" required="true" stored="true"/>
</schema>
----

=== SAX symbolic encoding

Note that the Chunk has aggregated fields. In addition to the classic statistics on the value and the quality of the point, we also integrate symbolic encoding with SAX.

The advantage of using SAX is that it is able to act as a dimensionality reduction tool, it tolerates time series of different lengths and makes it easier to find trends.

SAX encoding is a method used to simplify time series through a kind of summary of time intervals. By averaging, grouping and symbolically representing periods, the data becomes much smaller and easier to process, while still capturing its important aspects. For example, it can be used to detect statistical changes in trends and therefore abnormal behavior.


https://www.kdnuggets.com/2019/09/time-series-baseball.html
http://www.marc-boulle.fr/publications/BonduEtAlIJCNN13.pdf

---

