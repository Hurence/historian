{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272a9ad5-eb45-40ce-9a0f-c18fc09603e7",
   "metadata": {},
   "source": [
    "# Comparison of forecast algorithms for time series data\n",
    "\n",
    "This notebook's goal is to compare the different forecast algorithms implemented (found in the different directories). For this we will compare them using 3 different indicators:\n",
    "\n",
    "### Training time\n",
    "\n",
    "It is the time (in seconds) the algorithm takes to analyse the data and for the neural networks to modify the weights of the neurons.\n",
    "\n",
    "### Inference time\n",
    "\n",
    "It is the time the algorithm takes to predict the $X$ next data of the time series.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "To determine the accuracy of the diff√©rent prediction of our algorithm we will use the **Root Mean Square** method.  \n",
    "To calculate this indicator we need the data predected ($x_1$) and the real data ($x_2$), and we just have to use the following formula :\n",
    "<p align=\"center\">\n",
    "    <img width=\"200\" height=\"100\" src=\"rms.svg\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2066d6-aa25-4436-87ab-bab24426e19e",
   "metadata": {},
   "source": [
    "## 1) Insertions and definitions\n",
    "\n",
    "In this part we will insert all the library we will use in this jupyter notebook. We will also define all the usefull functions to alyse our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c56ae-e856-4547-9af6-540a256bb63a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_dataset(dnn):\n",
    "    # AutoRegressive Integrated Mooving Averge\n",
    "    arima_id = pd.read_csv('ARIMA/ARIMA_id.csv')\n",
    "    arima_diff = pd.read_csv('ARIMA/ARIMA_id2_diff.csv')\n",
    "    # Long Short-Term Memory\n",
    "    lstm_id = pd.read_csv('LSTM/LSTM_id2.csv')\n",
    "    lstm_name = pd.read_csv('LSTM/LSTM_name2.csv')\n",
    "    # Gated Recurrent Uniteddatadiff\n",
    "    gru_id = pd.read_csv('GRU/GRU_id2.csv')\n",
    "    gru_name = pd.read_csv('GRU/GRU_name2.csv')\n",
    "    # Recurrent Neural Network\n",
    "    rnn_id = pd.read_csv('RNN/RNN_id.csv')\n",
    "    rnn_name = pd.read_csv('RNN/RNN_name.csv')\n",
    "    # Deep Neural Network with lookback=10 and units=16,8\n",
    "    dnn_168_id = pd.read_csv('DNN/DNN_16_id.csv')\n",
    "    # Deep Neural Network with lookback=10 and 1st units=16,16\n",
    "    dnn_1616_id = pd.read_csv('DNN/DNN_1616_id.csv')\n",
    "    # Deep Neural Network with lookback=10\n",
    "    dnn_id = pd.read_csv('DNN/DNN_id.csv')\n",
    "    dnn_name = pd.read_csv('DNN/DNN_name.csv')\n",
    "    # Deep Neural Network with lookback=30\n",
    "    dnn_30_id = pd.read_csv('DNN/DNN_30_id2.csv')\n",
    "    dnn_30_name = pd.read_csv('DNN/DNN_30_name2.csv')\n",
    "    \n",
    "    # List of dataset\n",
    "    if not dnn:\n",
    "        dataset = [arima_id, arima_diff, lstm_id, lstm_name, gru_id, gru_name, dnn_id, dnn_name, rnn_id, rnn_name]  # , dnn_30_id, dnn_30_name, dnn_168_id, dnn_1616_id]\n",
    "        dataname = ['arima_id', 'arima_diff', 'lstm_id', 'lstm_name', 'gru_id', 'gru_name', 'dnn_id', 'dnn_name', 'rnn_id', 'rnn_name']  # , 'dnn_30_id', 'dnn_30_name', 'dnn_168_id', 'dnn_1616_id']\n",
    "    else:\n",
    "        dataset = [dnn_id, dnn_name, dnn_30_id, dnn_30_name, dnn_168_id, dnn_1616_id]\n",
    "        dataname = ['dnn_id', 'dnn_name', 'dnn_30_id', 'dnn_30_name', 'dnn_168_id', 'dnn_1616_id']\n",
    "    \n",
    "    return dataset, dataname\n",
    "\n",
    "def synthesis(dataset, dataname, string, decim, prin):\n",
    "    order = False\n",
    "    if string == 'order_time':\n",
    "        if prin:\n",
    "            print('Training time and time of order computing for arima comparaison (seconds) :')\n",
    "        string = 'training_time'\n",
    "        order = True\n",
    "    if (not order) and prin:\n",
    "        print('{} comparaison '.format(re.sub('_', ' ', string).capitalize()),end='')\n",
    "        if string[-5:] == '_time':\n",
    "            print('(seconds) ',end='')\n",
    "        print(':')\n",
    "    l = []\n",
    "    for j in range(len(dataset)):\n",
    "        cptr = 0\n",
    "        if string == 'training_time' and dataname[j][-5:] == '_name':\n",
    "            tmp = []\n",
    "            for i in range(0, dataset[j].shape[0]):\n",
    "                if dataset[0].loc[i,'metric_name'] not in tmp:\n",
    "                    tmp.append(dataset[0].loc[i,'metric_name'])\n",
    "                    cptr += dataset[j].loc[i,string]\n",
    "        else:\n",
    "            for i in range(0, dataset[j].shape[0]):\n",
    "                if order and  dataname[j][:5] == 'arima':\n",
    "                    cptr += dataset[j].loc[i,string] + dataset[0].loc[i,'order_time']\n",
    "                else:\n",
    "                    cptr += dataset[j].loc[i,string]\n",
    "        l.append([dataname[j], cptr/dataset[j].shape[0]])\n",
    "    l = sorted(l, key=lambda cptr: cptr[1])\n",
    "    if prin:\n",
    "        l1, l2 = l[:int(len(l)/2)], l[int(len(l)/2):]\n",
    "        space = '      '\n",
    "        for tup1, tup2 in zip(l1, l2):\n",
    "            print('{name:11} : {val:11} {spac} | {spac} {name2:11} : {val2:11}'.format(name=tup1[0], val=round(tup1[1], decim), spac=space, name2=tup2[0], val2=round(tup2[1], decim)))\n",
    "    return l\n",
    "\n",
    "def adjust(liste):\n",
    "    m = liste[-1][1]\n",
    "    res = []\n",
    "    for i in range(len(liste)):\n",
    "        res.append([liste[i][0], (liste[i][1]/m)*100])\n",
    "    return res\n",
    "        \n",
    "def autolabel(ax, rects, liste, n):\n",
    "    i = 0\n",
    "    for rect in rects:\n",
    "        h = rect.get_height()\n",
    "        ax.text(rect.get_x()+rect.get_width()/2., 5+h, '{}'.format(round(liste[i][1], n)),\n",
    "                ha='center', va='bottom', rotation=90)\n",
    "        i+=1\n",
    "\n",
    "# Comparaison id/name temps d'entrainement/RMS\n",
    "def comp_idName(algo, order_training, mean_squared):\n",
    "    for liste in order_training:\n",
    "        if algo.lower() + '_name' == liste[0]:\n",
    "            order_name = liste[1]\n",
    "        if algo.lower() + '_id' == liste[0]:\n",
    "            order_id = liste[1]\n",
    "    for liste in mean_squared:\n",
    "        if algo.lower() + '_name' == liste[0]:\n",
    "            mean_name = liste[1]\n",
    "        if algo.lower() + '_id' == liste[0]:\n",
    "            mean_id = liste[1]\n",
    "    print('\\tMean of training time of the {} algorithm :'.format(algo))\n",
    "    print(\"{first:55} : {second:5} seconds/metric_id\".format(first='Training on each mectric_id', second=round(order_id,2)))\n",
    "    print(\"{first:55} : {second:5} seconds/metric_id\".format(first='Training on a single metric_id for an entire metri_name', second=round(order_name,2)))\n",
    "    print('\\n\\tMean of root mean squared error of the {} algorithm :'.format(algo))\n",
    "    print(\"{first:55} : {second:5}\".format(first='Training on each mectric_id', second=round(mean_id,2)))\n",
    "    print(\"{first:55} : {second:5}\".format(first='Training on a single metric_id for an entire metri_name', second=round(mean_name,2)))\n",
    "\n",
    "    print('\\nTraining only on a single metric_id for an entire metri_name is {} time faster,\\nbut we are loosing {}% of precision'.format(round(order_id/order_name,2), round(100*mean_name/mean_id,2)))\n",
    "    \n",
    "def moustaches(dataset, dataname, string):\n",
    "    k=0\n",
    "    fig1, ax1 = plt.subplots(int(len(dataset)/2),2,figsize=(20,12))\n",
    "    for i in range(len(dataset)):\n",
    "        ax1[k,(i%2)].set_title('Horizontal Boxes of {}'.format(dataname[i]))\n",
    "        ax1[k,(i%2)].boxplot(dataset[i][[string]], vert=False)\n",
    "        if i%2==1:\n",
    "            k+=1\n",
    "    fig1.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def val_extr(data, string):\n",
    "    aber = []\n",
    "    for algo in data:\n",
    "        Q1 = np.quantile(algo[string], 0.25)\n",
    "        Q3 = np.quantile(algo[string], 0.75)\n",
    "        EIQ = Q3 - Q1\n",
    "        lim_inf = Q1 - 1.5*EIQ\n",
    "        lim_sup = Q3 + 1.5*EIQ\n",
    "        indx = list(algo[string].index[(algo[string] < lim_inf) | (lim_sup < algo[string])])\n",
    "        for x in indx:\n",
    "            if x not in aber:\n",
    "                aber.append(x)\n",
    "    for algo in data:\n",
    "        algo = algo.drop(sorted(aber), axis=0, inplace=True)\n",
    "    return sorted(aber)\n",
    "\n",
    "def val_aber(data, string):\n",
    "    for algo in data:\n",
    "        Q1 = np.quantile(algo[string], 0.25)\n",
    "        Q3 = np.quantile(algo[string], 0.75)\n",
    "        EIQ = Q3 - Q1\n",
    "        lim_inf = Q1 - 1.5*EIQ\n",
    "        lim_sup = Q3 + 1.5*EIQ\n",
    "        indx = list(algo[string].index[(algo[string] < lim_inf) | (lim_sup < algo[string])])\n",
    "        algo = algo.drop(indx, axis=0, inplace=True)\n",
    "\n",
    "def root_mean(dataset, dataname):\n",
    "    fig1, ax1 = plt.subplots(2,1,figsize=(12,8))\n",
    "    ax1[0].set_title('Root mean squared error of the different methods')\n",
    "    ax1[1].set_title('Root mean squared error of the different methods with logarithm scale')\n",
    "    ax1[0].grid(True,which=\"both\", linestyle='--')\n",
    "    ax1[1].grid(True,which=\"both\", linestyle='--')\n",
    "    for algo in dataset:\n",
    "        ax1[0].plot(algo['mean_squared_error'].to_numpy().reshape(-1,1), linewidth=1)\n",
    "        ax1[1].plot(algo['mean_squared_error'].to_numpy().reshape(-1,1), linewidth=1)\n",
    "    ax1[1].set_yscale('log')\n",
    "    fig1.legend(dataname)\n",
    "    fig1.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def preparation(mean_squared, inference, training, order_training):\n",
    "    adj_mean = adjust(mean_squared)\n",
    "    adj_infe = adjust(inference)\n",
    "    adj_train = adjust(training)\n",
    "    adj_order = adjust(order_training)\n",
    "\n",
    "    mean_squared = sorted(mean_squared, key=lambda cptr: cptr[0])\n",
    "    inference = sorted(inference, key=lambda cptr: cptr[0])\n",
    "    training = sorted(training, key=lambda cptr: cptr[0])\n",
    "    order_training = sorted(order_training, key=lambda cptr: cptr[0])\n",
    "    adj_mean = sorted(adj_mean, key=lambda cptr: cptr[0])\n",
    "    adj_infe = sorted(adj_infe, key=lambda cptr: cptr[0])\n",
    "    adj_train = sorted(adj_train, key=lambda cptr: cptr[0])\n",
    "    adj_order = sorted(adj_order, key=lambda cptr: cptr[0])\n",
    "    return [adj_mean, adj_infe, adj_train, adj_order, mean_squared, inference, training, order_training]\n",
    "    \n",
    "def histo(prep):\n",
    "    fig=plt.figure(figsize=(16,4), dpi= 100, facecolor='w', edgecolor='k')\n",
    "    ax = fig.add_subplot(111)\n",
    "    w = 0.2\n",
    "    N = len(prep[5])\n",
    "    ind = np.arange(N)\n",
    "    plt.ylim(0,150)\n",
    "    # plt.grid()\n",
    "\n",
    "    rect1 = ax.bar(ind+0*w, [tup[1] for tup in prep[0]], w)\n",
    "    rect2 = ax.bar(ind+1*w, [tup[1] for tup in prep[1]], w)\n",
    "    rect3 = ax.bar(ind+2*w, [tup[1] for tup in prep[2]], w)\n",
    "    rect4 = ax.bar(ind+3*w, [tup[1] for tup in prep[3]], w)\n",
    "\n",
    "    ax.set_ylabel('Percentages')\n",
    "    ax.set_xticks(ind+w)\n",
    "    ax.set_xticklabels([tup[0] for tup in prep[5]])\n",
    "    ax.legend(['Root mean squared error', 'Inference time', 'Training time', 'Training and order time'], loc='upper left')\n",
    "    plt.title('Synthesis of the indicators of each method')\n",
    "\n",
    "    autolabel(ax, rect1, prep[4], 2)\n",
    "    autolabel(ax, rect2, prep[5], 6)\n",
    "    autolabel(ax, rect3, prep[6], 2)\n",
    "    autolabel(ax, rect4, prep[7], 2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8dfd38-8247-4754-b501-5a725283e9ce",
   "metadata": {},
   "source": [
    "## 2) Dataset's preparation\n",
    "\n",
    "In this part we will import all the data we have collected to compare the algorithm and we will stock them in a list, in an other list we stock in the same order their name, it will be usefull for some functions and for some displays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d69ae-d2b4-4e94-92c1-b5ff52b6f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = False\n",
    "dataset, dataname = load_dataset(dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ad710-5bf7-46ff-954c-ed81aa8f587a",
   "metadata": {},
   "source": [
    "## 3) First analysis\n",
    "\n",
    "We are going to split data in function of the 3 different attributs and store them by croissant order in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c5916-a178-4c60-8c04-156d3c57b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared = synthesis(dataset, dataname, 'mean_squared_error', 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caaea8d-6f98-4d2f-b841-4dcbb44a3d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = synthesis(dataset, dataname, 'inference_time', 6, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb2fb1-abc2-448c-932a-fc6d48d00612",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = synthesis(dataset, dataname, 'training_time', 2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8fb5d3-cf8e-44d4-bcd8-4628974209cb",
   "metadata": {},
   "source": [
    "Here we have a problem with the training time indicator because *ARIMA*'s method has a very low training time because it is not a neural network, but in this algorithm there is a step that can be interpreted as the training of this method, it is the time needed to find the 3 parameters (p, d and q). So we create a new indicator; the training time and the time to find the order for *ARIMA*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb8b94-35bc-4394-bfe1-e6ee2697e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_training = synthesis(dataset, dataname, 'order_time', 2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2195425b-2c38-4bca-84cf-c360decd5ba5",
   "metadata": {},
   "source": [
    "We will summurize the four previous cells in a histogram graph to have a better view. To overpass the scale problems, we will set the biggest values of each indicator as 100% and the other are determinate in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53482a29-da61-4c5b-9fc6-47c14e06f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = preparation(mean_squared, inference, training, order_training)\n",
    "histo(prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc2cfd-6003-4453-82b3-dcc1344b58e4",
   "metadata": {},
   "source": [
    "The most important indicator is the accuracy one because even if an algorithm is the fastest, if its prediction are not accurate, it will be useless. So we will display the **RMS** indicator of each time series and for each algorithm.  \n",
    "First we can see that each method has spikes on the same time series, and secondly we can also see that the scale is not appropriate because this spikes are in big numbers ($10^7$). So we can display the same graph with a logarithm scale. Here we can see that apart from certain exceptions, the **RMS** indicator of each method are more or less under 1000 which means that the algorithms are pretty accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22818893-4d07-490f-bdd1-b13471a2dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean(dataset, dataname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d23b8d-dc24-4824-807f-627e44836a9e",
   "metadata": {},
   "source": [
    "We are therefore going to look at these extreme values using boxplots and we notice that our datasets have a lot of outliers. So much so that we don't even see the boxes. So we are going to reshape our data to do not take these outliers into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06efbe-52de-42c8-b4bc-9b135c7b7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'mean_squared_error'\n",
    "moustaches(dataset, dataname, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a12674-494c-44e3-a1e3-698462c0ded5",
   "metadata": {},
   "source": [
    "## 4) Second analysis: outliers in function of their methods\n",
    "\n",
    "We are going to follow the same steps but without taking into account the outliers.  \n",
    "In fact there is two ways to do this, we can delete all the outliers of each methods for all the methods we can delete the outliers of each methods just for the corresponding methods.  \n",
    "First we will delete the outliers of their correspouding algorithm. (I think that this method is the most significant).  \n",
    "Here we can see the boxplot so data are more significant, but of course we loose more data using this method, almost 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6098f7-885c-4b97-9de0-b4431cb98ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_aber(dataset, string)\n",
    "for algo in dataset:\n",
    "    algo.reset_index(inplace=True)\n",
    "moustaches(dataset, dataname, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73bc93-3128-4bb3-9186-444882e701d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean(dataset, dataname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552bffb-9ebb-4462-86cf-fc1ae42bf061",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared = synthesis(dataset, dataname, 'mean_squared_error', 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a320d-f043-407a-bc39-b4e4124832e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = synthesis(dataset, dataname, 'inference_time', 6, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf1ead-69e2-42b8-8994-52313771c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = synthesis(dataset, dataname, 'training_time', 2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ecc5f6-0ae7-48fa-afc9-c9225a3a3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_training = synthesis(dataset, dataname, 'order_time', 2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eac27b-a1f3-4764-ab0e-96d88bb24534",
   "metadata": {},
   "source": [
    "We can clearly see that without the outliers all methods are more accurate and faster to train but classement on some indicators have change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8642b35-c56c-4937-a7de-637d94c0bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = preparation(mean_squared, inference, training, order_training)\n",
    "histo(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e9d3a-5c24-428b-b49b-8ab57c97e780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comparaison DNN id/name temps d'entrainement/RMS\n",
    "# comp_idName('DNN', order_training, mean_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593109e-08a7-4552-8080-70cd3bd8fc50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comparaison DNN id/name temps d'entrainement/RMS\n",
    "# comp_idName('DNN_30', order_training, mean_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2099d-2f0c-479c-8f10-446ac6d58222",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5) Third analysis: delete outliers for all methods\n",
    "\n",
    "We will now reload the data and delete the outliers of all algorithm.\n",
    "Of course we loose more data using this method, almost 26%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a8c8b4-4472-45ae-a1c3-15c3dbbd3add",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset, dataname = load_dataset(dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf014855-07d9-45cb-9d07-0d02f361b14e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extr = val_extr(dataset, string)\n",
    "for algo in dataset:\n",
    "    algo.reset_index(inplace=True)\n",
    "moustaches(dataset, dataname, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a08f4-1c44-4460-ba58-5cebab884386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_mean(dataset, dataname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a69683-885c-45eb-b9d2-53a8087012ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_squared = synthesis(dataset, dataname, 'mean_squared_error', 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5281d-8c68-4320-8e2a-0103382514a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference = synthesis(dataset, dataname, 'inference_time', 6, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756f6a0-b749-4378-845c-b6bd9801ab5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training = synthesis(dataset, dataname, 'training_time', 2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717314bb-6373-468b-a4b0-0a43457c6375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "order_training = synthesis(dataset, dataname, 'order_time', 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e7499-995a-4532-bbc5-1b34b342c70f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prep = preparation(mean_squared, inference, training, order_training)\n",
    "histo(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e825db06-6bf4-45b5-b8cf-a0a13cedc645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comparaison LSTM id/name temps d'entrainement/RMS\n",
    "# comp_idName('LSTM', order_training, mean_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad77633-7a61-4730-86c2-1114acf217a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comparaison GRU id/name temps d'entrainement/RMS\n",
    "# comp_idName('GRU', order_training, mean_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bede21-d5c5-462b-9766-c9a23c752dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comparaison RNN id/name temps d'entrainement/RMS\n",
    "# comp_idName('RNN', order_training, mean_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a18ba-a413-4416-8afc-19fded1ba117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comparaison DNN id/name temps d'entrainement/RMS\n",
    "# comp_idName('DNN', order_training, mean_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c75df6-48dd-406e-b3e3-18a29940bc76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comparaison DNN id/name temps d'entrainement/RMS\n",
    "# comp_idName('DNN_30', order_training, mean_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b773fe6-1597-4502-a544-a197257334a9",
   "metadata": {},
   "source": [
    "## 6) Conclusion: which algorithm to choose ?\n",
    "\n",
    "We will conclude by using our second analysis, the one where we removed the outliers linked to their method. In my opinion, the results of this analysis are the most representative.  \n",
    "We can disting 3 algorithms, one for each indicators:\n",
    "- Training time: dnn_name\n",
    "- Inference time: arima_id\n",
    "- Accuracy: dnn_id\n",
    "\n",
    "The most important indicator is that of precision, but *dnn_name* is one of the least precise algorithms. If we forget about these two algorithms, then the new ones with the lowest training time are *gru_name* and *rnn_name* which are also not precise. Thus we then find *dnn_id* which is also the most precise and (after *ARIMA*) with the lowest inference time.  \n",
    "If we look at our first analysis, we can note that the results are exactly the same as ones of the second analysis. On another hand, the third scan has different results for the accuracy indicator, it is the *gru_id* algorithm which is the more accurate and in the second place we find *dnn_id*. But as we can see, the training time of  *gru_id* is 21 times longer than the *dnn_id*'s one. In the end, we can also find the same conclusion as the previous analysis.\n",
    "\n",
    "Now we can ask ourselves a new question, with which analysis window and with how many units is this method the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f6450-c4ae-415a-aeed-99dfdce179d0",
   "metadata": {},
   "source": [
    "After having tried different combinations between the loock back window and the composition of our neuron network, it turns out that the best arrangement found is the one with a look_back=10 and with 2 hidden layers of 16 neurons each."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
