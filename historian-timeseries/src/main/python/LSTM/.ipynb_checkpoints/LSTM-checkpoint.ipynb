{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82966443-bf9d-454c-b8c5-5c7d76344260",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory\n",
    "\n",
    "In this jupyter notebook you will find the implementation of the long short-term memory algorithm using the sklearn library. It will help to test this algorithm and to complete [forecasting.md](https://github.com/Hurence/historian/blob/forecasting/docs/forecasting.md) document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8527e4a-8fe4-49ab-82ed-63166daee5eb",
   "metadata": {},
   "source": [
    "First we need to import all the different libraries that we will use and we make the 'create_dataset' function, it will be used later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959f2b17-a213-4bdc-968f-1c652d00a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import sklearn.linear_model as sk\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "# LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa8aa6d-5088-4fec-a38f-6596458989c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7fc3d-94cf-41a7-8727-f03d48926693",
   "metadata": {},
   "source": [
    "#### 1) Dataset\n",
    "\n",
    "We prepare the dataset for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7921969a-5a28-416e-8587-fc87248c5bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>metric_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>091c334c-a90a-4d8f-ba75-2c936220cd64</td>\n",
       "      <td>1575157723</td>\n",
       "      <td>13.375</td>\n",
       "      <td>cpu_prct_used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>091c334c-a90a-4d8f-ba75-2c936220cd64</td>\n",
       "      <td>1575157423</td>\n",
       "      <td>13.500</td>\n",
       "      <td>cpu_prct_used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>091c334c-a90a-4d8f-ba75-2c936220cd64</td>\n",
       "      <td>1575157123</td>\n",
       "      <td>13.375</td>\n",
       "      <td>cpu_prct_used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>091c334c-a90a-4d8f-ba75-2c936220cd64</td>\n",
       "      <td>1575156823</td>\n",
       "      <td>13.500</td>\n",
       "      <td>cpu_prct_used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>091c334c-a90a-4d8f-ba75-2c936220cd64</td>\n",
       "      <td>1575156523</td>\n",
       "      <td>13.750</td>\n",
       "      <td>cpu_prct_used</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              metric_id   timestamp   value    metric_name\n",
       "0  091c334c-a90a-4d8f-ba75-2c936220cd64  1575157723  13.375  cpu_prct_used\n",
       "1  091c334c-a90a-4d8f-ba75-2c936220cd64  1575157423  13.500  cpu_prct_used\n",
       "2  091c334c-a90a-4d8f-ba75-2c936220cd64  1575157123  13.375  cpu_prct_used\n",
       "3  091c334c-a90a-4d8f-ba75-2c936220cd64  1575156823  13.500  cpu_prct_used\n",
       "4  091c334c-a90a-4d8f-ba75-2c936220cd64  1575156523  13.750  cpu_prct_used"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# ts_data = pd.read_csv('../data/dataHistorian.csv', sep=';', encoding='cp1252')\n",
    "ts_data = pd.read_csv('../data/it-data-4metrics.csv', sep=',')\n",
    "\n",
    "# Delete the useless columns\n",
    "ts_data = ts_data.iloc[:,0:4]\n",
    "ts_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c63d2-dc87-4a64-a8e4-8b55fea4d336",
   "metadata": {},
   "source": [
    "We create to dictionnary to class all the time series according to their metric_name and their metric_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33773db9-2c8e-4539-a347-d9bb3019926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the dictionnary of all the metric_name in association with their metric_id\n",
    "dic_name = {}\n",
    "dic_id = {}\n",
    "for indx in ts_data.index:\n",
    "    if ts_data['metric_name'][indx] not in dic_name.keys():\n",
    "        dic_name[ts_data['metric_name'][indx]] = []\n",
    "    if ts_data['metric_id'][indx] not in dic_name[ts_data['metric_name'][indx]]:\n",
    "        dic_name[ts_data['metric_name'][indx]].append(ts_data['metric_id'][indx])\n",
    "        dic_id[ts_data['metric_id'][indx]] = [ts_data['metric_name'][indx]]\n",
    "keys_name = list(dic_name.keys())\n",
    "keys_id = list(dic_id.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb1b68-f4ef-4a35-a70c-4744d47282df",
   "metadata": {},
   "source": [
    "#### 2) Training the neural network\n",
    "\n",
    "We are going to separate the series into a training and a testing serie (for each metric_id). Then this series are going to fit the neural network and to test it. We will store the results in dic_id for keep them and reuse them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8bef69f-3288-448f-96f6-bb3bd1609575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.88 % completed...\n",
      "19.76 % completed...\n",
      "29.64 % completed...\n",
      "39.53 % completed...\n",
      "49.41 % completed...\n",
      "59.29 % completed...\n",
      "69.17 % completed...\n",
      "79.05 % completed...\n",
      "88.93 % completed...\n",
      "98.81 % completed...\n"
     ]
    }
   ],
   "source": [
    "sample = len(keys_id)\n",
    "for i in range(sample):\n",
    "    indx = keys_id[i]\n",
    "    indexNames = ts_data[ ts_data['metric_id'] == indx ].index\n",
    "    data = ts_data.iloc[indexNames].sort_values(by='timestamp', ascending=True).loc[:,'value']\n",
    "    dataset = data.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    dic_id[indx].append(dataset)\n",
    "    \n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset.reshape(-1, 1))\n",
    "    \n",
    "    # split into train and test sets\n",
    "    train_size = int(len(dataset) * 0.67)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "    \n",
    "    # reshape into X=t and Y=t+1\n",
    "    look_back = 1\n",
    "    x_train, y_train = create_dataset(train, look_back)\n",
    "    x_valid, y_valid = create_dataset(test, look_back)\n",
    "    \n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    x_valid = np.reshape(x_valid, (x_valid.shape[0], 1, x_valid.shape[1]))\n",
    "    \n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    start_train = time.time()\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=1, verbose=0)\n",
    "    end_train = time.time()\n",
    "    \n",
    "    # make predictions\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    start_pred = time.time()\n",
    "    y_pred_valid = model.predict(x_valid)\n",
    "    end_pred = time.time()\n",
    "    \n",
    "    # invert predictions\n",
    "    y_pred_train = scaler.inverse_transform(y_pred_train)\n",
    "    y_train = scaler.inverse_transform([y_train])\n",
    "    y_pred_valid = scaler.inverse_transform(y_pred_valid)\n",
    "    y_valid = scaler.inverse_transform([y_valid])\n",
    "    # calculate root mean squared error\n",
    "    testScore = math.sqrt(mean_squared_error(y_valid[0], y_pred_valid[:,0]))\n",
    "    \n",
    "    dic_id[indx].append(testScore)\n",
    "    dic_id[indx].append([x_train, y_train, y_pred_train])\n",
    "    dic_id[indx].append([x_valid, y_valid, y_pred_valid])\n",
    "    dic_id[indx].append(end_train - start_train)\n",
    "    dic_id[indx].append(end_pred - start_pred)\n",
    "    if (i+1) % 25 == 0:\n",
    "        print(\"%.2f\" % ((100/sample)*(i+1)),\"% completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ccee15-d49e-4ca7-a22c-a921dbd10d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dic_id).loc[:,['metric_name', 'metric_id', 'mean_squarred_error', 'training_time', 'inference_time']].to_csv('LSTM_bis.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88f9a37b-61e7-473a-9e1c-91577ecb9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have two dictionaries:\n",
    "# First, we have a link between the metric_name and their metric_id\n",
    "\n",
    "# {'metric_name_1':[metric_id_1, metric_id_2, ...],\n",
    "#  'metric_name_2':[metric_id_1, metric_id_2, ...],\n",
    "#  ...}\n",
    "\n",
    "\n",
    "# Second, we have all the information according to the metric_id\n",
    "\n",
    "# {'metric_id_1':[metric_name_x, ts_data['value'], RMS, [x_train, y_train, y_pred_train], [x_valid, y_valid, y_pred_valid], training_time, inference_time],\n",
    "#  'metric_id_2':[metric_name_y, ts_data['value'], RMS, [x_train, y_train, y_pred_train], [x_valid, y_valid, y_pred_valid], training_time, inference_time],\n",
    "#  ...}\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d174aa-f394-442b-9f8a-937d8e44e88a",
   "metadata": {},
   "source": [
    "#### 3) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80e5da9b-3de5-40ed-bea8-2959914bdfc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c08b2bc1049e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindx_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdic_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindx_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msomme\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdic_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindx_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcptr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msomme\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for indx_name in keys_name:\n",
    "    somme = 0\n",
    "    cptr = 0\n",
    "    for indx_id in dic_name[indx_name]:\n",
    "        somme += dic_id[indx_id][2]\n",
    "        cptr += 1\n",
    "    l.append(somme/cptr)\n",
    "    \n",
    "dic = {'metric_name':keys_name, 'r2_mean':l}\n",
    "mean_error = pd.DataFrame(dic)\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dbe1e2-c5e9-4874-a5e4-5d7ab61cf78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(sample):\n",
    "    indx = keys_id[i]\n",
    "    fig, ax = plt.subplots()\n",
    "    # shift train predictions for plotting\n",
    "    trainPlot = np.empty_like(dic_id[indx][1].to_numpy().reshape(-1,1))\n",
    "    trainPlot[:, :] = np.nan\n",
    "    trainPlot[look_back:len(dic_id[indx][3][2])+look_back, :] = dic_id[indx][3][2]\n",
    "    # shift test predictions for plotting\n",
    "    validPlot = np.empty_like(dic_id[indx][1].to_numpy().reshape(-1,1))\n",
    "    validPlot[:, :] = np.nan\n",
    "    validPlot[len(dic_id[indx][3][2])+(look_back*2)+1:len(dic_id[indx][1])-1, :] = dic_id[indx][4][2]\n",
    "    # plot baseline and predictions\n",
    "    ax.plot(dic_id[indx][1].to_numpy().reshape(-1,1))\n",
    "    ax.plot(trainPlot)\n",
    "    ax.plot(validPlot)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
